{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transfer learning with YAMNet for cough sound classification\n\n[YAMNet](https://tfhub.dev/google/yamnet/1) is a pre-trained deep neural network that can predict audio events from [521 classes](https://github.com/tensorflow/models/blob/master/research/audioset/yamnet/yamnet_class_map.csv), such as laughter, barking, or a siren. \n\n In this tutorial you will learn how to:\n\n- Load and use the YAMNet model for inference.\n- Build a new model using the YAMNet embeddings to classify cat and dog sounds.\n- Evaluate and export your model.\n","metadata":{"id":"K2madPFAGHb3","papermill":{"duration":0.01569,"end_time":"2021-07-31T16:09:51.820682","exception":false,"start_time":"2021-07-31T16:09:51.804992","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Import TensorFlow and other libraries\n","metadata":{"id":"5Mdp2TpBh96Y","papermill":{"duration":0.015961,"end_time":"2021-07-31T16:09:51.85253","exception":false,"start_time":"2021-07-31T16:09:51.836569","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Start by installing [TensorFlow I/O](https://www.tensorflow.org/io), which will make it easier for you to load audio files off disk.","metadata":{"id":"zCcKYqu_hvKe","papermill":{"duration":0.015784,"end_time":"2021-07-31T16:09:51.884285","exception":false,"start_time":"2021-07-31T16:09:51.868501","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install tensorflow_io\n# !pip install tensorflow==1.15.2","metadata":{"executionInfo":{"elapsed":6691,"status":"ok","timestamp":1627649034073,"user":{"displayName":"K. M. Naimul Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf5gqW-OjoCPEL1Kziblj95wX7s7GnI1KTu5pcKE8=s64","userId":"03402884099506613426"},"user_tz":-360},"id":"urBpRWDHTHHU","outputId":"a6e266dc-8446-495c-c5b2-d2910cb6254b","papermill":{"duration":78.036015,"end_time":"2021-07-31T16:11:09.936252","exception":false,"start_time":"2021-07-31T16:09:51.900237","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nfrom IPython import display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_io as tfio\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, CSVLogger, EarlyStopping\nimport warnings","metadata":{"executionInfo":{"elapsed":360,"status":"ok","timestamp":1627649966817,"user":{"displayName":"K. M. Naimul Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf5gqW-OjoCPEL1Kziblj95wX7s7GnI1KTu5pcKE8=s64","userId":"03402884099506613426"},"user_tz":-360},"id":"7l3nqdWVF-kC","papermill":{"duration":2.372252,"end_time":"2021-07-31T16:11:12.639153","exception":false,"start_time":"2021-07-31T16:11:10.266901","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## About YAMNet\n\n[YAMNet](https://github.com/tensorflow/models/tree/master/research/audioset/yamnet) is a pre-trained neural network that employs the [MobileNetV1](https://arxiv.org/abs/1704.04861) depthwise-separable convolution architecture. It can use an audio waveform as input and make independent predictions for each of the 521 audio events from the [AudioSet](http://g.co/audioset) corpus.\n\nInternally, the model extracts \"frames\" from the audio signal and processes batches of these frames. This version of the model uses frames that are 0.96 second long and extracts one frame every 0.48 seconds .\n\nThe model accepts a 1-D float32 Tensor or NumPy array containing a waveform of arbitrary length, represented as single-channel (mono) 16 kHz samples in the range `[-1.0, +1.0]`. This tutorial contains code to help you convert WAV files into the supported format.\n\nThe model returns 3 outputs, including the class scores, embeddings (which you will use for transfer learning), and the log mel [spectrogram](https://www.tensorflow.org/tutorials/audio/simple_audio#spectrogram). You can find more details [here](https://tfhub.dev/google/yamnet/1).\n\nOne specific use of YAMNet is as a high-level feature extractor - the 1,024-dimensional embedding output. You will use the base (YAMNet) model's input features and feed them into your shallower model consisting of one hidden `tf.keras.layers.Dense` layer. Then, you will train the network on a small amount of data for audio classification _without_ requiring a lot of labeled data and training end-to-end. (This is similar to [transfer learning for image classification with TensorFlow Hub](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub) for more information.)\n\nFirst, you will test the model and see the results of classifying audio. You will then construct the data pre-processing pipeline.\n\n### Loading YAMNet from TensorFlow Hub\n\nYou are going to use a pre-trained YAMNet from [Tensorflow Hub](https://tfhub.dev/) to extract the embeddings from the sound files.\n\nLoading a model from TensorFlow Hub is straightforward: choose the model, copy its URL, and use the `load` function.\n\nNote: to read the documentation of the model, use the model URL in your browser.","metadata":{"id":"v9ZhybCnt_bM","papermill":{"duration":0.323712,"end_time":"2021-07-31T16:11:13.284177","exception":false,"start_time":"2021-07-31T16:11:12.960465","status":"completed"},"tags":[]}},{"cell_type":"code","source":"yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\nyamnet_model = hub.load(yamnet_model_handle)","metadata":{"executionInfo":{"elapsed":10663,"status":"ok","timestamp":1627639604023,"user":{"displayName":"K. M. Naimul Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf5gqW-OjoCPEL1Kziblj95wX7s7GnI1KTu5pcKE8=s64","userId":"03402884099506613426"},"user_tz":-360},"id":"06CWkBV5v3gr","papermill":{"duration":6.837105,"end_time":"2021-07-31T16:11:20.511119","exception":false,"start_time":"2021-07-31T16:11:13.674014","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You will need a function to load audio files, which will also be used later when working with the training data. (Learn more about reading audio files and their labels in [Simple audio recognition](https://www.tensorflow.org/tutorials/audio/simple_audio#reading_audio_files_and_their_labels).\n\nNote: The returned `wav_data` from `load_wav_16k_mono` is already normalized to values in the `[-1.0, 1.0]` range (for more information, go to [YAMNet's documentation on TF Hub](https://tfhub.dev/google/yamnet/1)).","metadata":{"id":"mBm9y9iV2U_-","papermill":{"duration":0.296781,"end_time":"2021-07-31T16:11:21.103848","exception":false,"start_time":"2021-07-31T16:11:20.807067","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Utility functions for loading audio files and making sure the sample rate is correct.\n\n@tf.function\ndef load_wav_16k_mono(filename):\n    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n    file_contents = tf.io.read_file(filename)\n    wav, sample_rate = tf.audio.decode_wav(\n          file_contents,\n          desired_channels=1)\n    wav = tf.squeeze(wav, axis=-1)\n    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n    return wav","metadata":{"executionInfo":{"elapsed":362,"status":"ok","timestamp":1627650022305,"user":{"displayName":"K. M. Naimul Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf5gqW-OjoCPEL1Kziblj95wX7s7GnI1KTu5pcKE8=s64","userId":"03402884099506613426"},"user_tz":-360},"id":"Xwc9Wrdg2EtY","papermill":{"duration":0.323898,"end_time":"2021-07-31T16:11:21.725788","exception":false,"start_time":"2021-07-31T16:11:21.40189","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the class mapping\n\nIt's important to load the class names that YAMNet is able to recognize. The mapping file is present at `yamnet_model.class_map_path()` in the CSV format.","metadata":{"id":"6z6rqlEz20YB","papermill":{"duration":0.296731,"end_time":"2021-07-31T16:11:22.326899","exception":false,"start_time":"2021-07-31T16:11:22.030168","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class_map_path = yamnet_model.class_map_path().numpy().decode('utf-8')\nclass_names =list(pd.read_csv(class_map_path)['display_name'])\n\nfor name in class_names[:20]:\n  print(name)\nprint('...')","metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1627639604024,"user":{"displayName":"K. M. Naimul Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf5gqW-OjoCPEL1Kziblj95wX7s7GnI1KTu5pcKE8=s64","userId":"03402884099506613426"},"user_tz":-360},"id":"6Gyj23e_3Mgr","outputId":"df0f10f5-f211-4cbb-f93c-cb7cfde248e0","papermill":{"duration":0.337537,"end_time":"2021-07-31T16:11:22.959905","exception":false,"start_time":"2021-07-31T16:11:22.622368","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Settings: Explore the data\n","metadata":{"id":"qcruxiuX1cO5","papermill":{"duration":0.298206,"end_time":"2021-07-31T16:11:23.553386","exception":false,"start_time":"2021-07-31T16:11:23.25518","status":"completed"},"tags":[]}},{"cell_type":"code","source":"pos_label= 'Cough'\nneg_label= 'Speech'\nbase_data_path= '../input/privacy-aware-cough-event-detection/eval/eval/'\n# base_data_path= '../input/privacy-aware-cough-event-detection/soundscapes/soundscapes/'\nsaved_model_path = '../input/privacy-aware-cough-event-detection/pretrained_'+pos_label+'_detection_3/pretrained_'+pos_label+'_detection_3'\n# saved_model_path = '../input/privacy-aware-cough-event-detection/pretrained_'+pos_label+'_detection_with_speech/pretrained_'+pos_label+'_detection_with_speech'","metadata":{"papermill":{"duration":0.305485,"end_time":"2021-07-31T16:11:24.160254","exception":false,"start_time":"2021-07-31T16:11:23.854769","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filter the data\n\nNow that the data is stored in the `DataFrame`, apply some transformations:\n\n- Filter out rows and use only the selected classes - `Cough` and `Non-Cough`. If you want to use any other classes, this is where you can choose them.\n- Amend the filename to have the full path. This will make loading easier later.\n- Change targets to be within a specific range. In this example, `Non-Cough` will be `0`, and `Cough` will become `1`.","metadata":{"id":"7d4rHBEQ2QAU","papermill":{"duration":0.343626,"end_time":"2021-07-31T16:11:25.658944","exception":false,"start_time":"2021-07-31T16:11:25.315318","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# cd_csv = base_data_path+pos_label+'_Detection_eval.csv'\ncd_csv = '../input/privacy-aware-cough-event-detection/'+'Cough_Detection_eval.csv'\npd_data = pd.read_csv(cd_csv)\npd_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_classes = [neg_label, pos_label]\nmap_class_to_id = {neg_label:0, pos_label:1}\n\nfiltered_pd = pd_data[pd_data.label.isin(my_classes)]\n\nclass_id = filtered_pd['label'].apply(lambda name: map_class_to_id[name])\nfiltered_pd = filtered_pd.assign(target=class_id)\n\nfull_path = filtered_pd['name'].apply(lambda row: os.path.join(base_data_path, row))\nfiltered_pd = filtered_pd.assign(name=full_path)\n\n\nfiltered_pd.head(10)","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1627649123481,"user":{"displayName":"K. M. Naimul Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf5gqW-OjoCPEL1Kziblj95wX7s7GnI1KTu5pcKE8=s64","userId":"03402884099506613426"},"user_tz":-360},"id":"tFnEoQjgs14I","outputId":"d06879e4-09e0-4000-a1e3-2ace46f72860","papermill":{"duration":0.350536,"end_time":"2021-07-31T16:11:26.30766","exception":false,"start_time":"2021-07-31T16:11:25.957124","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split the data\n\n","metadata":{"id":"ZdfPIeD0Qedk","papermill":{"duration":0.298318,"end_time":"2021-07-31T16:11:26.908461","exception":false,"start_time":"2021-07-31T16:11:26.610143","status":"completed"},"tags":[]}},{"cell_type":"code","source":"reloaded_model = tf.saved_model.load(saved_model_path)","metadata":{"executionInfo":{"elapsed":15718,"status":"ok","timestamp":1627649238041,"user":{"displayName":"K. M. Naimul Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf5gqW-OjoCPEL1Kziblj95wX7s7GnI1KTu5pcKE8=s64","userId":"03402884099506613426"},"user_tz":-360},"id":"KkYVpJS72WWB","papermill":{"duration":5.975042,"end_time":"2021-07-31T16:11:33.784255","exception":false,"start_time":"2021-07-31T16:11:27.809213","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And for the final test: given some sound data, does your model return the correct result?","metadata":{"id":"4BkmvvNzq49l","papermill":{"duration":0.294353,"end_time":"2021-07-31T16:11:34.481977","exception":false,"start_time":"2021-07-31T16:11:34.187624","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test_df= filtered_pd.copy()\ndivide_by=1\nTP=0\nTN=0\nFP=0\nFN=0\nP=0\nN=0\nfor i in range (int(len(test_df)/divide_by)):\n  testing_wav_label= test_df.iloc[i,1]\n  if testing_wav_label== pos_label:\n    P=P+1\n  else:\n    N=N+1\nfor i in range (int(len(test_df)/divide_by)):\n  testing_wav_file_name= test_df.iloc[i,0]\n  testing_wav_label= test_df.iloc[i,1]\n  testing_wav_data= load_wav_16k_mono(testing_wav_file_name)\n  reloaded_results = reloaded_model(testing_wav_data)\n  prediction = my_classes[tf.argmax(reloaded_results)]\n#   print(prediction)\n  if prediction== testing_wav_label:\n    if prediction== pos_label:\n      TP=TP+1\n    else:\n      TN=TN+1\n  if prediction!= testing_wav_label:\n    if prediction== pos_label:\n      FP=FP+1\n    else:\n      FN=FN+1\nrecall=0\nprecision=0\nf1=0\naccuracy=0\nif P>0:\n    recall= TP/P\nif TP+FP>0:\n    precision= TP/(TP+FP)\nif precision+recall>0:\n    f1= (2*precision*recall)/(precision+recall)\nif P+N>0:\n    accuracy= (TP+TN)/(P+N)\n    \nprint(\"TP, P, TN, N:\",TP, P, TN, N)\nprint(\"Recall:\",recall,\"\\n\")\nprint(\"Precision:\",precision,\"\\n\")\nprint(\"F1:\",f1,\"\\n\")\nprint(\"Accuracy:\",accuracy,\"\\n\")","metadata":{"executionInfo":{"elapsed":688187,"status":"ok","timestamp":1627650804089,"user":{"displayName":"K. M. Naimul Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf5gqW-OjoCPEL1Kziblj95wX7s7GnI1KTu5pcKE8=s64","userId":"03402884099506613426"},"user_tz":-360},"id":"xeXtD5HO28y-","outputId":"31a97884-27fe-42e1-a598-b5e200a509e6","papermill":{"duration":21.473549,"end_time":"2021-07-31T16:11:56.267454","exception":false,"start_time":"2021-07-31T16:11:34.793905","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you want to try your new model on a serving setup, you can use the 'serving_default' signature.","metadata":{"id":"ZRrOcBYTUgwn","papermill":{"duration":0.310794,"end_time":"2021-07-31T16:11:56.88981","exception":false,"start_time":"2021-07-31T16:11:56.579016","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# serving_results = reloaded_model.signatures['serving_default'](testing_wav_data)\n# cat_or_dog = my_classes[tf.argmax(serving_results['classifier'])]\n# print(f'The main sound is: {cat_or_dog}')\n","metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1627642304686,"user":{"displayName":"K. M. Naimul Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf5gqW-OjoCPEL1Kziblj95wX7s7GnI1KTu5pcKE8=s64","userId":"03402884099506613426"},"user_tz":-360},"id":"ycC8zzDSUG2s","papermill":{"duration":0.323155,"end_time":"2021-07-31T16:11:57.523327","exception":false,"start_time":"2021-07-31T16:11:57.200172","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (Optional) Some more testing\n\nThe model is ready.\n\nLet's compare it to YAMNet on the test dataset.","metadata":{"id":"da7blblCHs8c","papermill":{"duration":0.321409,"end_time":"2021-07-31T16:11:58.373755","exception":false,"start_time":"2021-07-31T16:11:58.052346","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# test_pd = filtered_pd.loc[filtered_pd['fold'] == 5]\n# row = test_pd.sample(1)\n# filename = row['filename'].item()\n# print(filename)\n# waveform = load_wav_16k_mono(filename)\n# print(f'Waveform values: {waveform}')\n# _ = plt.plot(waveform)\n\n# display.Audio(waveform, rate=16000)","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1627642304686,"user":{"displayName":"K. M. Naimul Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf5gqW-OjoCPEL1Kziblj95wX7s7GnI1KTu5pcKE8=s64","userId":"03402884099506613426"},"user_tz":-360},"id":"vDf5MASIIN1z","papermill":{"duration":0.319109,"end_time":"2021-07-31T16:11:59.003359","exception":false,"start_time":"2021-07-31T16:11:58.68425","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Run the model, check the output.\n# scores, embeddings, spectrogram = yamnet_model(waveform)\n# class_scores = tf.reduce_mean(scores, axis=0)\n# top_class = tf.argmax(class_scores)\n# inferred_class = class_names[top_class]\n# top_score = class_scores[top_class]\n# print(f'[YAMNet] The main sound is: {inferred_class} ({top_score})')\n\n# reloaded_results = reloaded_model(waveform)\n# your_top_class = tf.argmax(reloaded_results)\n# your_inferred_class = my_classes[your_top_class]\n# class_probabilities = tf.nn.softmax(reloaded_results, axis=-1)\n# your_top_score = class_probabilities[your_top_class]\n# print(f'[Your model] The main sound is: {your_inferred_class} ({your_top_score})')","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1627642304687,"user":{"displayName":"K. M. Naimul Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf5gqW-OjoCPEL1Kziblj95wX7s7GnI1KTu5pcKE8=s64","userId":"03402884099506613426"},"user_tz":-360},"id":"eYUzFxYJIcE1","papermill":{"duration":0.333993,"end_time":"2021-07-31T16:11:59.669596","exception":false,"start_time":"2021-07-31T16:11:59.335603","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Next steps\n\nYou have created a model that can classify sounds from dogs or cats. With the same idea and a different dataset you can try, for example, building an [acoustic identifier of birds](https://www.kaggle.com/c/birdclef-2021/) based on their singing.\n\nShare your project with the TensorFlow team on social media!\n","metadata":{"id":"g8Tsym8Rq-0V","papermill":{"duration":0.336466,"end_time":"2021-07-31T16:12:00.330721","exception":false,"start_time":"2021-07-31T16:11:59.994255","status":"completed"},"tags":[]}}]}